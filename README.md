Abstract:
You are an elf named Oilill who must make his journey towards the end city of Evereska. By starting in the small town of Loudwater, you must travel city to city to reach your end goal while keeping enough money for travels. Along your way, it is possible to encounter bandits that you must fight off in order to progress to the next city. If you win against the encounter, you can continue along your route to the next city. If you lose, you will lose money in return, for it has been robbed from you. In a world that is randomly generated with different cities and routes each time, as well as terrain and maps, every time you play will be different. By choosing the paths that you take wisely, you might be able to reach your final destination. Traveling between cities costs money, but money can always be earned by finding tasks that need done within cities (passive income). With the use of AI, both you and your opponents can make better decisions for you

AI components:
Within this project, two AI components are utilized. The first component is used as an enemy for the player. By learning and fighting with a hardcoded computer, the AI opponent model learns through the Monte Carlo method, which consists of acting out many iterations of random battles. After battling many times, these fights are recorded as well as the states these fights where taken in. From these examples, the AI opponent was able to “learn” what strategies worked for each case and which strategies did not. By using this method, the AI opponent can take a basic strategy and take countermeasures against it, as well as take in any input from the user and use it against them. The Second AI model is one for the player. before playing, the player can enable the AI within the code to allow the AI player model to take over travel and battles with the other AI model, which could provide useful in some situations. The Player AI, similarly uses mock battles to train itself with encounters. As for movement, Dijkstra’s shortest path algorithm is used, as well as a money management system in the event that it loses a battle or takes long to reach a certain destination.


Problems solved:
	The AI opponent solves the problem of using hard coded values for battle encounters with the player, while at the same time not being a complete random number generator. This gives some advantage to the player since the AI can pick up and learn new strategies. The player AI component Can solve the problem of encounters with the opponent AI, in the case that the human player does not want to manually fight with the AI opponent. The player AI is also capable of moving between cities and eventually reaching the end city of Evereska, effectively making it capable of playing the game in its entirety. This can provide useful if the human player does not know what to do or is unsure of what move to make next to save on resources. The AI player can also be used if for some reason that the human player just does not want to play the game. Both of these problems are solved with the usage of AI.

Other Components used:
	Map and city generation were created using a mix of Perlin noise and fitness models. The map is generated by using Perlin noise on a plane with a color map to create terrain that looks better than random pixels with different levels of elevation. City generation was done using a fitness model. By randomly placing cities on the map many times, the best placement is determined. Values on the map with lesser elevation (in the water) are given low fitness values, preventing them from being chosen. Alternatively, city values on the map with overly high values (on top of mountains) were also removed. City configurations with cites overly close to each other were also removed. Using both of these methods provides a different starting layout for each game, while also picking a better alternative starting position each time through.
	Routes were created by shuffling all the permutations of possible routes then taking ten routes at random. These routes are then checked to make sure there is a starting city to go to and a path to reach the end city. The routes are also checked to prevent a path directly from the first city to the last city, to prevent the player from winning quickly. If such a route exists, it is removed from the total list of routes created. Player are only allowed to travel on these routes and cannot travel where there is no route, to prevent easy winning.
	The battle encounters are similar to a game of rock, paper, scissors. Instead, you are given the choice between using a sword, arrow or fire. Sword nullifies sword attacks, beats fire, and takes damage against arrows. Arrows beat sword, loses to fire, and both players take damage if arrows are chosen for both. Fire beats arrows and loses to sword. If both players pick fire, they both take damage. Using these guidelines for the battle encounters, the AI models are able to learn what attacks work best in each scenario, depending on the health of the player and opponent.

 Additional notes:
	When you load into the game, you can press any key 0-9 to travel to that city if it is adjacent to your current city. As you travel, you will lose money naturally. Staying in a city for an extend duration can give you more money if needed. When engaging in combat, choose between arrows(a button), sword(s button) or fire(f button). Pressing these will do that attack and the output will be sent to the terminal. This continues until one player runs out of health. Within the code, there are several commented out sections of code, since their implementation does not work as intended. As of now, the biggest problem in implementing the movement of the player using AI and will not function as intended if the code is uncommented. Apart from that, most other implementations to the game should work as intended. The game is started by running the rode within lab 11's, agent_environment.py
